# üîç ML Workflow Pipeline ‚Äì Interactive & Modular Python Package

This project provides a **professional, extensible Python package** for building complete machine learning workflows‚Äîfrom raw data to trained models‚Äîguided via an intuitive **Jupyter Notebook interface**. It supports both classical ML and deep learning models.

---

## üìÅ Folder Structure

```
ML_Workflow/
‚îÇ
‚îú‚îÄ‚îÄ file_handler.py              # Load/save data from file or MySQL
‚îú‚îÄ‚îÄ data_understanding.py        # Explore shape, types, nulls, stats
‚îú‚îÄ‚îÄ preprocessing.py             # Clean, scale, and encode data
‚îú‚îÄ‚îÄ data_visualization.py        # Plot histograms, heatmaps, etc.
‚îú‚îÄ‚îÄ data_labeling.py             # Label encoding & target column handler
‚îú‚îÄ‚îÄ model_training.py            # Classical ML training & evaluation
‚îú‚îÄ‚îÄ deep_learning_training.py    # Deep learning model training (DNN, TabNet, FTTransformer)
‚îú‚îÄ‚îÄ model_saver.py               # Save/load trained models (.pkl)
‚îú‚îÄ‚îÄ ML_Workflow_Interface.ipynb  # üéØ Guided Jupyter Notebook interface
‚îî‚îÄ‚îÄ README.md
```

---

## üéØ Project Goal

To create a **modular, user-driven machine learning pipeline** that:

- Loads data from files or a MySQL database  
- Supports full preprocessing with visualization  
- Offers both classical ML and deep learning model training  
- Auto-detects classification/regression problems  
- Supports dynamic target column labeling and one-hot decoding  
- Saves trained models for deployment  

---

## üõ†Ô∏è Features

‚úÖ Load data from CSV, Excel, or MySQL  
‚úÖ Optional MySQL storage via SQLAlchemy + PyMySQL  
‚úÖ Interactive preprocessing (null handling, encoding, scaling)  
‚úÖ Rich visualization: histograms, boxplots, heatmaps, etc.  
‚úÖ Label target with label encoding or one-hot (with auto-recovery)  
‚úÖ Train both classical and deep learning models  
‚úÖ Save models using a class-based `.pkl` saver  
‚úÖ Modular architecture: plug in new models or steps easily  
‚úÖ Global memory tracking of user choices (e.g., target column)  

---

## ü§ñ Supported Models

### Supervised Learning

#### üìä Classical ML
- **Classification**: Logistic Regression, Decision Tree, Random Forest, SVM, LightGBM  
- **Regression**: Linear Regression, Decision Tree, Random Forest, SVR

#### üß† Deep Learning
- DNNTrainer (Fully Connected Deep Network)  
- TabNetTrainer (Tabular Attention-based Network)  
- FTTransformerTrainer (Requires `rtdl` and `torch`)

> You choose which model to use interactively in the notebook.

---

## üì¶ Outputs

- Cleaned, encoded, and scaled DataFrame
- Trained model (classical or deep learning)
- `.pkl` file for deployment (`model_saver.py`)
- Optional logs or metrics printed in notebook
- Optional Gantt chart if task scheduling is involved

---

## üíª How to Use

### 1. Clone the Repository

```bash
git clone https://github.com/your-username/ML_Workflow.git
cd ML_Workflow
```

### 2. Install Dependencies

```bash
pip install -r requirements.txt
```

**Includes:** `pandas`, `numpy`, `matplotlib`, `seaborn`, `scikit-learn`, `sqlalchemy`, `pymysql`, `lightgbm`, `torch`, `rtdl`, etc.

### 3. Launch the Notebook Interface

```bash
jupyter notebook ML_Workflow_Interface.ipynb
```

### 4. Follow the Guided Steps

- Step 1: Load or store data (file/DB)  
- Step 2‚Äì4: Explore and preprocess  
- Step 5: Label target column (user-specified and stored globally)  
- Step 6: Train ML model (classification/regression, auto-detected or manual)  
- Step 7‚Äì9: Train deep learning model (DNN/TabNet/FTTransformer on demand)  
- Step 10: Save model  

---

## üß™ Example Usage (Notebook Snippets)

```python
# Load & explore
df = load_data()
display_data_summary(df)

# Preprocess
preprocessed_df = preprocess_pipeline(df, scale_method='minmax')

# Label
labeler = DataLabeler(preprocessed_df)
target_column = input("Enter target column: ")
labeled_df = labeler.label_data(target_column)

# Train ML
model = MLModelPipeline(
    df=labeled_df,
    target_column=target_column,
    problem_type="classification",  # or auto
    model_choice="lightgbm"
).run()

# Train Deep Learning (choose one interactively)
trainer = DNNTrainer(labeled_df, target_column, epochs=30)
model = trainer.train()

# Save
ModelSaver("final_model.pkl").save(model)
```

---

## üß∞ Optional: MySQL Database Integration

- Load or store datasets via MySQL
- Use `.env` or prompted credentials
- Secure, SQLAlchemy-backed connection

---

## üöß Future Enhancements

- Unsupervised learning (KMeans, PCA, Isolation Forest)
- Streamlit or Gradio interface
- Docker + MLFlow support
- YAML-based config runner for automation

---

## ü§ù Contributing

Pull requests are welcome! Feel free to fork, enhance, or suggest improvements. All contributions will be reviewed with ‚ù§Ô∏è.

---

## üìú License

MIT License ‚Äì free for personal or commercial use.

---


